{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "add_regularization_l2_dropout.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UL5YSimH0NP9"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, regularizers\n",
        "from tensorflow.keras.datasets import cifar10"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train , y_train) , (x_test, y_test) = cifar10.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MT0pjAA-1Xqb",
        "outputId": "20b0c702-2659-46df-b214-0d6b532c4aa9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0"
      ],
      "metadata": {
        "id": "3_JJp69y1k9d"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#functional api\n",
        "def my_model():\n",
        "  inputs = keras.Input(shape=(32,32,3))\n",
        "  x = layers.Conv2D(32, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(inputs)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.MaxPool2D()(x)\n",
        "  x = layers.Conv2D(64, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.MaxPool2D()(x)\n",
        "  x = layers.Conv2D(128, 3, padding='same', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = keras.activations.relu(x)\n",
        "  x = layers.Flatten()(x)\n",
        "  x = layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(10)(x)\n",
        "  model = keras.Model(inputs = inputs, outputs=outputs)\n",
        "  return model"
      ],
      "metadata": {
        "id": "AtyJAC-h12W_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = my_model()\n",
        "model.compile(\n",
        "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer = keras.optimizers.Adam(lr = 3e-4),\n",
        "    metrics = ['accuracy']\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44IcmMya6PGq",
        "outputId": "1a839089-79d4-4099-edee-2caddbaa2bf8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, batch_size=64, epochs=150, verbose=2)\n",
        "model.evaluate(x_test, y_test, batch_size=64, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUqL00ym6rwu",
        "outputId": "a4c152e9-53ed-4edd-cb38-e0a9d4607670"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "782/782 - 15s - loss: 3.0632 - accuracy: 0.3585 - 15s/epoch - 20ms/step\n",
            "Epoch 2/150\n",
            "782/782 - 3s - loss: 1.9311 - accuracy: 0.4754 - 3s/epoch - 4ms/step\n",
            "Epoch 3/150\n",
            "782/782 - 3s - loss: 1.6240 - accuracy: 0.5239 - 3s/epoch - 4ms/step\n",
            "Epoch 4/150\n",
            "782/782 - 3s - loss: 1.4985 - accuracy: 0.5511 - 3s/epoch - 4ms/step\n",
            "Epoch 5/150\n",
            "782/782 - 3s - loss: 1.4221 - accuracy: 0.5709 - 3s/epoch - 4ms/step\n",
            "Epoch 6/150\n",
            "782/782 - 3s - loss: 1.3675 - accuracy: 0.5861 - 3s/epoch - 4ms/step\n",
            "Epoch 7/150\n",
            "782/782 - 3s - loss: 1.3386 - accuracy: 0.6002 - 3s/epoch - 4ms/step\n",
            "Epoch 8/150\n",
            "782/782 - 3s - loss: 1.3116 - accuracy: 0.6087 - 3s/epoch - 4ms/step\n",
            "Epoch 9/150\n",
            "782/782 - 4s - loss: 1.2941 - accuracy: 0.6137 - 4s/epoch - 4ms/step\n",
            "Epoch 10/150\n",
            "782/782 - 4s - loss: 1.2683 - accuracy: 0.6245 - 4s/epoch - 5ms/step\n",
            "Epoch 11/150\n",
            "782/782 - 4s - loss: 1.2565 - accuracy: 0.6309 - 4s/epoch - 5ms/step\n",
            "Epoch 12/150\n",
            "782/782 - 4s - loss: 1.2448 - accuracy: 0.6344 - 4s/epoch - 5ms/step\n",
            "Epoch 13/150\n",
            "782/782 - 3s - loss: 1.2313 - accuracy: 0.6424 - 3s/epoch - 4ms/step\n",
            "Epoch 14/150\n",
            "782/782 - 3s - loss: 1.2220 - accuracy: 0.6431 - 3s/epoch - 4ms/step\n",
            "Epoch 15/150\n",
            "782/782 - 3s - loss: 1.2084 - accuracy: 0.6505 - 3s/epoch - 4ms/step\n",
            "Epoch 16/150\n",
            "782/782 - 3s - loss: 1.1995 - accuracy: 0.6564 - 3s/epoch - 4ms/step\n",
            "Epoch 17/150\n",
            "782/782 - 3s - loss: 1.1886 - accuracy: 0.6616 - 3s/epoch - 4ms/step\n",
            "Epoch 18/150\n",
            "782/782 - 3s - loss: 1.1845 - accuracy: 0.6642 - 3s/epoch - 4ms/step\n",
            "Epoch 19/150\n",
            "782/782 - 3s - loss: 1.1670 - accuracy: 0.6699 - 3s/epoch - 4ms/step\n",
            "Epoch 20/150\n",
            "782/782 - 3s - loss: 1.1524 - accuracy: 0.6758 - 3s/epoch - 4ms/step\n",
            "Epoch 21/150\n",
            "782/782 - 3s - loss: 1.1474 - accuracy: 0.6783 - 3s/epoch - 4ms/step\n",
            "Epoch 22/150\n",
            "782/782 - 3s - loss: 1.1474 - accuracy: 0.6808 - 3s/epoch - 4ms/step\n",
            "Epoch 23/150\n",
            "782/782 - 3s - loss: 1.1295 - accuracy: 0.6846 - 3s/epoch - 4ms/step\n",
            "Epoch 24/150\n",
            "782/782 - 4s - loss: 1.1281 - accuracy: 0.6885 - 4s/epoch - 4ms/step\n",
            "Epoch 25/150\n",
            "782/782 - 3s - loss: 1.1243 - accuracy: 0.6905 - 3s/epoch - 4ms/step\n",
            "Epoch 26/150\n",
            "782/782 - 4s - loss: 1.1118 - accuracy: 0.6973 - 4s/epoch - 4ms/step\n",
            "Epoch 27/150\n",
            "782/782 - 3s - loss: 1.1100 - accuracy: 0.6982 - 3s/epoch - 4ms/step\n",
            "Epoch 28/150\n",
            "782/782 - 4s - loss: 1.1070 - accuracy: 0.6987 - 4s/epoch - 4ms/step\n",
            "Epoch 29/150\n",
            "782/782 - 4s - loss: 1.0973 - accuracy: 0.7039 - 4s/epoch - 4ms/step\n",
            "Epoch 30/150\n",
            "782/782 - 3s - loss: 1.0920 - accuracy: 0.7061 - 3s/epoch - 4ms/step\n",
            "Epoch 31/150\n",
            "782/782 - 4s - loss: 1.0888 - accuracy: 0.7070 - 4s/epoch - 4ms/step\n",
            "Epoch 32/150\n",
            "782/782 - 4s - loss: 1.0753 - accuracy: 0.7104 - 4s/epoch - 4ms/step\n",
            "Epoch 33/150\n",
            "782/782 - 4s - loss: 1.0713 - accuracy: 0.7157 - 4s/epoch - 5ms/step\n",
            "Epoch 34/150\n",
            "782/782 - 3s - loss: 1.0630 - accuracy: 0.7168 - 3s/epoch - 4ms/step\n",
            "Epoch 35/150\n",
            "782/782 - 4s - loss: 1.0597 - accuracy: 0.7186 - 4s/epoch - 5ms/step\n",
            "Epoch 36/150\n",
            "782/782 - 3s - loss: 1.0586 - accuracy: 0.7192 - 3s/epoch - 4ms/step\n",
            "Epoch 37/150\n",
            "782/782 - 3s - loss: 1.0503 - accuracy: 0.7235 - 3s/epoch - 4ms/step\n",
            "Epoch 38/150\n",
            "782/782 - 4s - loss: 1.0545 - accuracy: 0.7243 - 4s/epoch - 4ms/step\n",
            "Epoch 39/150\n",
            "782/782 - 3s - loss: 1.0419 - accuracy: 0.7288 - 3s/epoch - 4ms/step\n",
            "Epoch 40/150\n",
            "782/782 - 3s - loss: 1.0456 - accuracy: 0.7279 - 3s/epoch - 4ms/step\n",
            "Epoch 41/150\n",
            "782/782 - 3s - loss: 1.0373 - accuracy: 0.7308 - 3s/epoch - 4ms/step\n",
            "Epoch 42/150\n",
            "782/782 - 4s - loss: 1.0330 - accuracy: 0.7312 - 4s/epoch - 4ms/step\n",
            "Epoch 43/150\n",
            "782/782 - 4s - loss: 1.0319 - accuracy: 0.7334 - 4s/epoch - 4ms/step\n",
            "Epoch 44/150\n",
            "782/782 - 3s - loss: 1.0218 - accuracy: 0.7378 - 3s/epoch - 4ms/step\n",
            "Epoch 45/150\n",
            "782/782 - 3s - loss: 1.0268 - accuracy: 0.7323 - 3s/epoch - 4ms/step\n",
            "Epoch 46/150\n",
            "782/782 - 3s - loss: 1.0134 - accuracy: 0.7402 - 3s/epoch - 4ms/step\n",
            "Epoch 47/150\n",
            "782/782 - 4s - loss: 1.0133 - accuracy: 0.7397 - 4s/epoch - 4ms/step\n",
            "Epoch 48/150\n",
            "782/782 - 3s - loss: 1.0183 - accuracy: 0.7387 - 3s/epoch - 4ms/step\n",
            "Epoch 49/150\n",
            "782/782 - 3s - loss: 0.9990 - accuracy: 0.7456 - 3s/epoch - 4ms/step\n",
            "Epoch 50/150\n",
            "782/782 - 4s - loss: 1.0090 - accuracy: 0.7428 - 4s/epoch - 4ms/step\n",
            "Epoch 51/150\n",
            "782/782 - 3s - loss: 0.9983 - accuracy: 0.7469 - 3s/epoch - 4ms/step\n",
            "Epoch 52/150\n",
            "782/782 - 3s - loss: 1.0097 - accuracy: 0.7421 - 3s/epoch - 4ms/step\n",
            "Epoch 53/150\n",
            "782/782 - 3s - loss: 0.9962 - accuracy: 0.7463 - 3s/epoch - 4ms/step\n",
            "Epoch 54/150\n",
            "782/782 - 3s - loss: 0.9970 - accuracy: 0.7500 - 3s/epoch - 4ms/step\n",
            "Epoch 55/150\n",
            "782/782 - 3s - loss: 0.9954 - accuracy: 0.7456 - 3s/epoch - 4ms/step\n",
            "Epoch 56/150\n",
            "782/782 - 4s - loss: 0.9956 - accuracy: 0.7504 - 4s/epoch - 4ms/step\n",
            "Epoch 57/150\n",
            "782/782 - 3s - loss: 0.9950 - accuracy: 0.7497 - 3s/epoch - 4ms/step\n",
            "Epoch 58/150\n",
            "782/782 - 3s - loss: 0.9765 - accuracy: 0.7543 - 3s/epoch - 4ms/step\n",
            "Epoch 59/150\n",
            "782/782 - 4s - loss: 0.9858 - accuracy: 0.7544 - 4s/epoch - 5ms/step\n",
            "Epoch 60/150\n",
            "782/782 - 3s - loss: 0.9807 - accuracy: 0.7565 - 3s/epoch - 4ms/step\n",
            "Epoch 61/150\n",
            "782/782 - 3s - loss: 0.9840 - accuracy: 0.7551 - 3s/epoch - 4ms/step\n",
            "Epoch 62/150\n",
            "782/782 - 3s - loss: 0.9817 - accuracy: 0.7558 - 3s/epoch - 4ms/step\n",
            "Epoch 63/150\n",
            "782/782 - 3s - loss: 0.9772 - accuracy: 0.7572 - 3s/epoch - 4ms/step\n",
            "Epoch 64/150\n",
            "782/782 - 3s - loss: 0.9757 - accuracy: 0.7592 - 3s/epoch - 4ms/step\n",
            "Epoch 65/150\n",
            "782/782 - 3s - loss: 0.9657 - accuracy: 0.7634 - 3s/epoch - 4ms/step\n",
            "Epoch 66/150\n",
            "782/782 - 3s - loss: 0.9727 - accuracy: 0.7601 - 3s/epoch - 4ms/step\n",
            "Epoch 67/150\n",
            "782/782 - 3s - loss: 0.9645 - accuracy: 0.7630 - 3s/epoch - 4ms/step\n",
            "Epoch 68/150\n",
            "782/782 - 3s - loss: 0.9646 - accuracy: 0.7636 - 3s/epoch - 4ms/step\n",
            "Epoch 69/150\n",
            "782/782 - 3s - loss: 0.9620 - accuracy: 0.7622 - 3s/epoch - 4ms/step\n",
            "Epoch 70/150\n",
            "782/782 - 3s - loss: 0.9610 - accuracy: 0.7642 - 3s/epoch - 4ms/step\n",
            "Epoch 71/150\n",
            "782/782 - 3s - loss: 0.9557 - accuracy: 0.7658 - 3s/epoch - 4ms/step\n",
            "Epoch 72/150\n",
            "782/782 - 3s - loss: 0.9602 - accuracy: 0.7657 - 3s/epoch - 4ms/step\n",
            "Epoch 73/150\n",
            "782/782 - 3s - loss: 0.9654 - accuracy: 0.7628 - 3s/epoch - 4ms/step\n",
            "Epoch 74/150\n",
            "782/782 - 4s - loss: 0.9533 - accuracy: 0.7687 - 4s/epoch - 5ms/step\n",
            "Epoch 75/150\n",
            "782/782 - 3s - loss: 0.9548 - accuracy: 0.7690 - 3s/epoch - 4ms/step\n",
            "Epoch 76/150\n",
            "782/782 - 3s - loss: 0.9599 - accuracy: 0.7677 - 3s/epoch - 4ms/step\n",
            "Epoch 77/150\n",
            "782/782 - 4s - loss: 0.9477 - accuracy: 0.7691 - 4s/epoch - 4ms/step\n",
            "Epoch 78/150\n",
            "782/782 - 3s - loss: 0.9546 - accuracy: 0.7694 - 3s/epoch - 4ms/step\n",
            "Epoch 79/150\n",
            "782/782 - 3s - loss: 0.9445 - accuracy: 0.7756 - 3s/epoch - 4ms/step\n",
            "Epoch 80/150\n",
            "782/782 - 3s - loss: 0.9466 - accuracy: 0.7733 - 3s/epoch - 4ms/step\n",
            "Epoch 81/150\n",
            "782/782 - 3s - loss: 0.9455 - accuracy: 0.7762 - 3s/epoch - 4ms/step\n",
            "Epoch 82/150\n",
            "782/782 - 3s - loss: 0.9399 - accuracy: 0.7753 - 3s/epoch - 4ms/step\n",
            "Epoch 83/150\n",
            "782/782 - 4s - loss: 0.9438 - accuracy: 0.7784 - 4s/epoch - 4ms/step\n",
            "Epoch 84/150\n",
            "782/782 - 3s - loss: 0.9370 - accuracy: 0.7774 - 3s/epoch - 4ms/step\n",
            "Epoch 85/150\n",
            "782/782 - 3s - loss: 0.9376 - accuracy: 0.7786 - 3s/epoch - 4ms/step\n",
            "Epoch 86/150\n",
            "782/782 - 4s - loss: 0.9333 - accuracy: 0.7804 - 4s/epoch - 5ms/step\n",
            "Epoch 87/150\n",
            "782/782 - 3s - loss: 0.9387 - accuracy: 0.7783 - 3s/epoch - 4ms/step\n",
            "Epoch 88/150\n",
            "782/782 - 3s - loss: 0.9376 - accuracy: 0.7793 - 3s/epoch - 4ms/step\n",
            "Epoch 89/150\n",
            "782/782 - 4s - loss: 0.9204 - accuracy: 0.7843 - 4s/epoch - 4ms/step\n",
            "Epoch 90/150\n",
            "782/782 - 3s - loss: 0.9295 - accuracy: 0.7833 - 3s/epoch - 4ms/step\n",
            "Epoch 91/150\n",
            "782/782 - 3s - loss: 0.9268 - accuracy: 0.7834 - 3s/epoch - 4ms/step\n",
            "Epoch 92/150\n",
            "782/782 - 3s - loss: 0.9273 - accuracy: 0.7844 - 3s/epoch - 4ms/step\n",
            "Epoch 93/150\n",
            "782/782 - 3s - loss: 0.9360 - accuracy: 0.7808 - 3s/epoch - 4ms/step\n",
            "Epoch 94/150\n",
            "782/782 - 3s - loss: 0.9307 - accuracy: 0.7838 - 3s/epoch - 4ms/step\n",
            "Epoch 95/150\n",
            "782/782 - 4s - loss: 0.9239 - accuracy: 0.7865 - 4s/epoch - 5ms/step\n",
            "Epoch 96/150\n",
            "782/782 - 4s - loss: 0.9234 - accuracy: 0.7861 - 4s/epoch - 4ms/step\n",
            "Epoch 97/150\n",
            "782/782 - 4s - loss: 0.9264 - accuracy: 0.7854 - 4s/epoch - 4ms/step\n",
            "Epoch 98/150\n",
            "782/782 - 4s - loss: 0.9211 - accuracy: 0.7872 - 4s/epoch - 5ms/step\n",
            "Epoch 99/150\n",
            "782/782 - 4s - loss: 0.9211 - accuracy: 0.7890 - 4s/epoch - 4ms/step\n",
            "Epoch 100/150\n",
            "782/782 - 4s - loss: 0.9242 - accuracy: 0.7873 - 4s/epoch - 5ms/step\n",
            "Epoch 101/150\n",
            "782/782 - 4s - loss: 0.9269 - accuracy: 0.7841 - 4s/epoch - 4ms/step\n",
            "Epoch 102/150\n",
            "782/782 - 3s - loss: 0.9173 - accuracy: 0.7888 - 3s/epoch - 4ms/step\n",
            "Epoch 103/150\n",
            "782/782 - 4s - loss: 0.9155 - accuracy: 0.7897 - 4s/epoch - 5ms/step\n",
            "Epoch 104/150\n",
            "782/782 - 3s - loss: 0.9217 - accuracy: 0.7896 - 3s/epoch - 4ms/step\n",
            "Epoch 105/150\n",
            "782/782 - 3s - loss: 0.9155 - accuracy: 0.7897 - 3s/epoch - 4ms/step\n",
            "Epoch 106/150\n",
            "782/782 - 3s - loss: 0.9110 - accuracy: 0.7913 - 3s/epoch - 4ms/step\n",
            "Epoch 107/150\n",
            "782/782 - 3s - loss: 0.9199 - accuracy: 0.7907 - 3s/epoch - 4ms/step\n",
            "Epoch 108/150\n",
            "782/782 - 3s - loss: 0.9171 - accuracy: 0.7908 - 3s/epoch - 4ms/step\n",
            "Epoch 109/150\n",
            "782/782 - 3s - loss: 0.9139 - accuracy: 0.7911 - 3s/epoch - 4ms/step\n",
            "Epoch 110/150\n",
            "782/782 - 4s - loss: 0.9079 - accuracy: 0.7939 - 4s/epoch - 4ms/step\n",
            "Epoch 111/150\n",
            "782/782 - 3s - loss: 0.9115 - accuracy: 0.7941 - 3s/epoch - 4ms/step\n",
            "Epoch 112/150\n",
            "782/782 - 4s - loss: 0.9075 - accuracy: 0.7953 - 4s/epoch - 5ms/step\n",
            "Epoch 113/150\n",
            "782/782 - 3s - loss: 0.9155 - accuracy: 0.7906 - 3s/epoch - 4ms/step\n",
            "Epoch 114/150\n",
            "782/782 - 3s - loss: 0.9133 - accuracy: 0.7946 - 3s/epoch - 4ms/step\n",
            "Epoch 115/150\n",
            "782/782 - 3s - loss: 0.9033 - accuracy: 0.7973 - 3s/epoch - 4ms/step\n",
            "Epoch 116/150\n",
            "782/782 - 3s - loss: 0.9089 - accuracy: 0.7946 - 3s/epoch - 4ms/step\n",
            "Epoch 117/150\n",
            "782/782 - 4s - loss: 0.9095 - accuracy: 0.7954 - 4s/epoch - 4ms/step\n",
            "Epoch 118/150\n",
            "782/782 - 3s - loss: 0.9135 - accuracy: 0.7954 - 3s/epoch - 4ms/step\n",
            "Epoch 119/150\n",
            "782/782 - 3s - loss: 0.9082 - accuracy: 0.7961 - 3s/epoch - 4ms/step\n",
            "Epoch 120/150\n",
            "782/782 - 4s - loss: 0.9099 - accuracy: 0.7957 - 4s/epoch - 4ms/step\n",
            "Epoch 121/150\n",
            "782/782 - 3s - loss: 0.9061 - accuracy: 0.7972 - 3s/epoch - 4ms/step\n",
            "Epoch 122/150\n",
            "782/782 - 4s - loss: 0.9015 - accuracy: 0.7981 - 4s/epoch - 4ms/step\n",
            "Epoch 123/150\n",
            "782/782 - 3s - loss: 0.9009 - accuracy: 0.7991 - 3s/epoch - 4ms/step\n",
            "Epoch 124/150\n",
            "782/782 - 3s - loss: 0.9112 - accuracy: 0.7971 - 3s/epoch - 4ms/step\n",
            "Epoch 125/150\n",
            "782/782 - 4s - loss: 0.9100 - accuracy: 0.7939 - 4s/epoch - 4ms/step\n",
            "Epoch 126/150\n",
            "782/782 - 3s - loss: 0.8965 - accuracy: 0.8007 - 3s/epoch - 4ms/step\n",
            "Epoch 127/150\n",
            "782/782 - 3s - loss: 0.9024 - accuracy: 0.7995 - 3s/epoch - 4ms/step\n",
            "Epoch 128/150\n",
            "782/782 - 3s - loss: 0.9040 - accuracy: 0.7994 - 3s/epoch - 4ms/step\n",
            "Epoch 129/150\n",
            "782/782 - 3s - loss: 0.9055 - accuracy: 0.7977 - 3s/epoch - 4ms/step\n",
            "Epoch 130/150\n",
            "782/782 - 3s - loss: 0.8977 - accuracy: 0.8001 - 3s/epoch - 4ms/step\n",
            "Epoch 131/150\n",
            "782/782 - 3s - loss: 0.8987 - accuracy: 0.8004 - 3s/epoch - 4ms/step\n",
            "Epoch 132/150\n",
            "782/782 - 3s - loss: 0.9046 - accuracy: 0.7992 - 3s/epoch - 4ms/step\n",
            "Epoch 133/150\n",
            "782/782 - 3s - loss: 0.9028 - accuracy: 0.7979 - 3s/epoch - 4ms/step\n",
            "Epoch 134/150\n",
            "782/782 - 3s - loss: 0.9067 - accuracy: 0.7975 - 3s/epoch - 4ms/step\n",
            "Epoch 135/150\n",
            "782/782 - 3s - loss: 0.8921 - accuracy: 0.8019 - 3s/epoch - 4ms/step\n",
            "Epoch 136/150\n",
            "782/782 - 3s - loss: 0.8988 - accuracy: 0.7999 - 3s/epoch - 4ms/step\n",
            "Epoch 137/150\n",
            "782/782 - 4s - loss: 0.8951 - accuracy: 0.8026 - 4s/epoch - 4ms/step\n",
            "Epoch 138/150\n",
            "782/782 - 4s - loss: 0.8968 - accuracy: 0.8021 - 4s/epoch - 4ms/step\n",
            "Epoch 139/150\n",
            "782/782 - 3s - loss: 0.8960 - accuracy: 0.8022 - 3s/epoch - 4ms/step\n",
            "Epoch 140/150\n",
            "782/782 - 3s - loss: 0.8928 - accuracy: 0.8011 - 3s/epoch - 4ms/step\n",
            "Epoch 141/150\n",
            "782/782 - 3s - loss: 0.8992 - accuracy: 0.8013 - 3s/epoch - 4ms/step\n",
            "Epoch 142/150\n",
            "782/782 - 3s - loss: 0.8926 - accuracy: 0.8046 - 3s/epoch - 4ms/step\n",
            "Epoch 143/150\n",
            "782/782 - 3s - loss: 0.8924 - accuracy: 0.8041 - 3s/epoch - 4ms/step\n",
            "Epoch 144/150\n",
            "782/782 - 3s - loss: 0.8897 - accuracy: 0.8026 - 3s/epoch - 4ms/step\n",
            "Epoch 145/150\n",
            "782/782 - 3s - loss: 0.8976 - accuracy: 0.8031 - 3s/epoch - 4ms/step\n",
            "Epoch 146/150\n",
            "782/782 - 3s - loss: 0.8950 - accuracy: 0.8035 - 3s/epoch - 4ms/step\n",
            "Epoch 147/150\n",
            "782/782 - 4s - loss: 0.8951 - accuracy: 0.8029 - 4s/epoch - 4ms/step\n",
            "Epoch 148/150\n",
            "782/782 - 4s - loss: 0.8968 - accuracy: 0.8025 - 4s/epoch - 4ms/step\n",
            "Epoch 149/150\n",
            "782/782 - 3s - loss: 0.8954 - accuracy: 0.8042 - 3s/epoch - 4ms/step\n",
            "Epoch 150/150\n",
            "782/782 - 3s - loss: 0.8918 - accuracy: 0.8061 - 3s/epoch - 4ms/step\n",
            "157/157 - 1s - loss: 1.1495 - accuracy: 0.7355 - 618ms/epoch - 4ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.1495380401611328, 0.7354999780654907]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OmNU_vy_7GyN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}